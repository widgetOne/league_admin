# -*- coding: utf-8 -*-
"""Final 2025 Google ortools softball schedule maker 2025-05-05.ipynb

Automatically generated by Colab.

Original file is located at
# Removed before sync 2025-05-09
"""

#@title import stuff N@
import pandas as pd
import random
import numpy as np
pd.options.mode.chained_assignment = None  # default='warn'
from copy import deepcopy
from pprint import pprint
import logging
import io
import os
import re
import shutil
import datetime

FINAL_EXPORT = False

# Copy of Google ortools softball schedule maker 2025-05-04b  backup of 2025-05-04 19:53.ipynb
# Google ortools softball schedule maker 2025-05-04b.ipynb

# Removed before sync 2025-05-09

template_path = os.path.join(root_path, template_relative_path)
template_full_path = os.path.join(template_path, template_name)
template_local_path = os.path.join('./', template_name)

if not os.path.isfile(template_local_path):
  shutil.copy(template_full_path, template_local_path)

print(template_local_path)
print(os.listdir(template_path))
print(f'source file exists: {os.path.isfile(template_full_path)}')
print(f'local  file exists: {os.path.isfile(template_local_path)}')



!pip install gspread pandas gspread-pandas



from IPython.display import Audio, display, Javascript

# Optional: play a chime
def notify_done():
    display(Javascript('''
        (function() {
          const context = new AudioContext();
          const o = context.createOscillator();
          const g = context.createGain();
          o.type = 'sine';
          o.frequency.value = 880;
          o.connect(g);
          g.connect(context.destination);
          o.start();
          g.gain.exponentialRampToValueAtTime(0.0001, context.currentTime + 1);
        })();
    '''))

# prompt: I want to download a file from my local computer and parse it to a pandas dataframe

import pandas as pd
from google.colab import files
import io

asdf = '''
uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
'''

with open(template_local_path, 'r') as tf:
  template_str = tf.read()

# Read the entire file content into a string
#uploaded_str = uploaded[fn].decode('utf-8')
uploaded_str = template_str

# Split the string into lines
lines = uploaded_str.splitlines()

# Remove the first 16 lines
modified_lines = []
for line in lines[16:]:
  parts = line.split(',')
  modified_lines.append(','.join(parts[:10])) # Keep only the first 10 columns

# Join the remaining lines back into a single string
modified_content = '\n'.join(modified_lines)
print(modified_lines[:2])
# Join the remaining lines back into a single string
modified_content = '\n'.join(modified_lines)
# Assuming the uploaded file is a CSV
csv_dtypes = {'field setup': 'Int64'}
download_df = pd.read_csv(io.StringIO(modified_content), dtype=csv_dtypes)

#download_df = pd.read_csv(modified_content, dtype={'Unnamed: 9': 'Int64'})  # Replace 'fn' with the actual filename if different
print(download_df.head())

download_df.iloc[17]

# prompt: I want to do the following: copy download_df into new df schedule_content. I want to rename all the column headers to snake case. I want to change teh date column into date objects from the us standard date they are in. I want to replace all the NaN in field setup with zeros.

import pandas as pd

# Assuming download_df is already defined from the previous code

schedule_content = download_df.copy()

schedule_content = schedule_content.iloc[:113]
# Rename columns to snake case
def to_snake_case(name):
    return name.replace(' ', '_').lower()

new_cols = {col: to_snake_case(col) for col in schedule_content.columns}
print(new_cols)
schedule_content = schedule_content.rename(columns=new_cols)

del_idx = 42
del_row = schedule_content.iloc[del_idx]
#print(del_row)
schedule_content = schedule_content.drop(del_idx)
schedule_content = schedule_content.reset_index(drop=True)

# Convert date column to datetime objects
schedule_content['date'] = pd.to_datetime(schedule_content['date'], format='%m/%d/%Y').map(lambda x: x.date())
schedule_content['weekend_idx'] = schedule_content.weekend_idx.astype(int)

# Replace NaN in 'field_setup' with 0
schedule_content['field_setup'] = schedule_content['field_setup'].fillna(0)

schedule_content["time_idx"] = schedule_content.groupby(["date", "field"]).cumcount()
schedule_content["max_time_idx"] = schedule_content.groupby(["date", "field"])["time_idx"].transform("max")


print(len(schedule_content))
print(schedule_content.head(2))

####. !pip install pulp

#balancing sat and sunday
#inderdivisional play is flexible
#balancing setup is flexible

####. import pulp

def match_idx(row):
  return (
      row.weekend_idx,
      row.date,
      row.field,
      row.time,
      row.time_idx,
      row.max_time_idx,
      row.field_setup,
    )
tup_to_match_idx = lambda idx_row_tup: match_idx(idx_row_tup[1])

print(schedule_content.iloc[0])
print(match_idx(schedule_content.iloc[0]))



weekend_idxs = schedule_content.weekend_idx.unique()
dates = schedule_content.date.unique()
fields = schedule_content.field.unique()
times = schedule_content.time.unique()
matches = list(map(tup_to_match_idx, schedule_content.iterrows()))

# Removed before sync 2025-05-09

teams = list(str, range(14))

team_to_idx = {name: idx for idx, name in enumerate(teams)}
num_teams = len(teams)
print(weekend_idxs)

vs_play_base = [
 [0, 4, 4, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0],
 [4, 0, 4, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0],
 [4, 4, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0],
 [2, 2, 2, 0, 3, 2, 2, 3, 0, 0, 0, 0, 0, 0],
 [2, 2, 2, 3, 0, 3, 2, 2, 0, 0, 0, 0, 0, 0],
 [2, 2, 2, 2, 3, 0, 3, 2, 0, 0, 0, 0, 0, 0],
 [2, 2, 2, 2, 2, 3, 0, 3, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 3, 2, 2, 3, 0, 1, 1, 1, 1, 1, 1],
 [0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 3, 3, 3, 3],
 [0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 3, 3, 3, 3],
 [0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 0, 3, 3, 3],
 [0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 0, 3, 3],
 [0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 3, 0, 3],
 [0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 3, 3, 0]]

for row_idx in range(0, 14):
  assert sum(vs_play_base[row_idx]) == 16

for col_idx in range(0, 14):
  col_total = sum(vs_play_base[row_idx][col_idx] for row_idx in range(0, 14))
  assert col_total == 16


vs_totals = {}
for team1_idx, team1 in enumerate(teams):
  vs_totals[team1] = {}
  for team2_idx, team2 in enumerate(teams):
    vs_totals[team1][team2] = vs_play_base[team1_idx][team2_idx]

# vs_totals

!pip install ortools

# this was teh first drtaf using the old boolean based logic
#


from ortools.sat.python import cp_model
import datetime

model = cp_model.CpModel()

#############################################################################
# define basic model

home_team = {}
away_team = {}

for m in matches:
    home_team[m] = model.NewIntVar(0, len(teams) - 1, f"home_team_{m}")
    away_team[m] = model.NewIntVar(0, len(teams) - 1, f"away_team_{m}")

    # Basic constraint: no team plays against itself
    model.Add(home_team[m] != away_team[m])

#############################################################################
# define layered booleans needed in constraints

is_home = {}
is_away = {}
is_playing = {}

for m in matches:
    for t_idx in range(len(teams)):
        # is_home[m, t_idx]
        is_home[m, t_idx] = model.NewBoolVar(f"is_home_{m}_{t_idx}")
        model.Add(home_team[m] == t_idx).OnlyEnforceIf(is_home[m, t_idx])
        model.Add(home_team[m] != t_idx).OnlyEnforceIf(is_home[m, t_idx].Not())

        # is_away[m, t_idx]
        is_away[m, t_idx] = model.NewBoolVar(f"is_away_{m}_{t_idx}")
        model.Add(away_team[m] == t_idx).OnlyEnforceIf(is_away[m, t_idx])
        model.Add(away_team[m] != t_idx).OnlyEnforceIf(is_away[m, t_idx].Not())

        # is_playing[m, t_idx] = is_home OR is_away
        is_playing[m, t_idx] = model.NewBoolVar(f"is_playing_{m}_{t_idx}")
        model.AddBoolOr([is_home[m, t_idx], is_away[m, t_idx]]).OnlyEnforceIf(is_playing[m, t_idx])
        model.AddBoolAnd([is_home[m, t_idx].Not(), is_away[m, t_idx].Not()]).OnlyEnforceIf(is_playing[m, t_idx].Not())


match_time_ints_for_min = {}
match_time_ints_for_max = {}

min_time_idx = 0
max_time_idx = max(m[4] for m in matches)

for t_idx in range(len(teams)):
    for m in matches:
        time_idx = m[4]  # assuming m[4] is the time_idx of the match
        var = model.NewIntVar(min_time_idx, max_time_idx + 1, f"min_match_time_int_{m}_{t_idx}")
        model.Add(var == time_idx).OnlyEnforceIf(is_playing[m, t_idx])
        model.Add(var == max_time_idx + 1).OnlyEnforceIf(is_playing[m, t_idx].Not())
        match_time_ints_for_min[m, t_idx] = var

        var = model.NewIntVar(min_time_idx, max_time_idx + 1, f"max_match_time_int_{m}_{t_idx}")
        model.Add(var == time_idx).OnlyEnforceIf(is_playing[m, t_idx])
        model.Add(var == min_time_idx).OnlyEnforceIf(is_playing[m, t_idx].Not())
        match_time_ints_for_max[m, t_idx] = var

#############################################################################
# define layered booleans needed in constraints

# total play of 16
for t_idx in range(len(teams)):
    model.Add(sum(is_playing[m, t_idx] for m in matches) == 16)

# weekend play targets
total_play_source = [1,2,2,1,2,2,2,2,2]
for w in weekend_idxs:
  play_target = total_play_source[w-1]
  for t_idx in range(len(teams)):
      model.Add(sum(
          is_playing[m, t_idx] for m in matches if m[0] == w
      ) == play_target)

# home and away should be balanced
for t_idx in range(len(teams)):
    model.Add(sum(is_home[m, t_idx] for m in matches) == 8)
    model.Add(sum(is_away[m, t_idx] for m in matches) == 8)

# each team only plays on one field on one day, each weekend
#
plays_on_field = {}  # (w, date, t_idx, field) -> BoolVar

for t_idx in range(len(teams)):
  for w in range(1, 10):
      field_day_flags = []
      date_field_pairs = set((m[1], m[2]) for m in matches if m[0] == w)
      for date, field in date_field_pairs:

        flag = model.NewBoolVar(f"plays_weekend{w}_date{date}_team{t_idx}_field{field}")
        plays_on_field[w, date, field, t_idx] = flag

        # If team plays any match on field f during weekend w → flag = 1
        relevant_matches = [m for m in matches if m[0] == w and m[1] == date and m[2] == field]
        match_flags = [is_playing[m, t_idx] for m in relevant_matches]

        if match_flags:
            model.AddMaxEquality(flag, match_flags)
            field_day_flags.append(flag)

      # At most one field per weekend for this team
      if field_day_flags:
          model.Add(sum(field_day_flags) <= 1)


# each team plays the correct number of times per day
total_play_source = [1,2,2,1,2,2,2,2,2]
for w in weekend_idxs:
  for t_idx in range(len(teams)):
      model.Add(sum(is_home[m, t_idx] for m in matches if m[0] == w) <= 1)
      model.Add(sum(is_away[m, t_idx] for m in matches if m[0] == w) <= 1)


# Make sure that teams are not sitting for more than 1 game (i.e. manage idle time)
for t_idx in range(len(teams)):
    for m1 in matches:
        date1, field1, idx1 = m1[1], m1[2], m1[4]

        for m2 in matches:
            if m1 >= m2:  # we only need one set of these constaints
                continue
            date2, field2, idx2 = m2[1], m2[2], m2[4]

            # Only consider matches on the same field/date
            if date1 != date2 or field1 != field2:
                continue

            # If time difference is 3 or more, team cannot play both
            if abs(idx1 - idx2) >= 3:
                # Add: is_playing[m1, t_idx] + is_playing[m2, t_idx] ≤ 1
                model.Add(is_playing[m1, t_idx] + is_playing[m2, t_idx] <= 1)

#############################################################################
# vs balance
#

vs_match_flags_storage = {}

for t1 in teams:
    for t2 in teams:
        if t1 == t2:
            continue
        if t2 not in vs_totals.get(t1, {}):
            continue

        t1_idx = team_to_idx[t1]
        t2_idx = team_to_idx[t2]
        expected = vs_totals[t1][t2]

        if t1_idx >= t2_idx:
            continue
        # Each match counts if t1 is home and t2 is away, or vice versa

        vs_match_flags = []

        for m in matches:
            vs_match_idx = (t1_idx, t2_idx) + m

            # is_home[m, t1] AND is_away[m, t2]
            # OR is_home[m, t2] AND is_away[m, t1]

            both_1 = model.NewBoolVar(f"vs_{t1}_home_{t2}_away_in_{m}")
            model.AddBoolAnd([is_home[m, t1_idx], is_away[m, t2_idx]]).OnlyEnforceIf(both_1)
            model.AddBoolOr([is_home[m, t1_idx].Not(), is_away[m, t2_idx].Not()]).OnlyEnforceIf(both_1.Not())

            both_2 = model.NewBoolVar(f"vs_{t2}_home_{t1}_away_in_{m}")
            model.AddBoolAnd([is_home[m, t2_idx], is_away[m, t1_idx]]).OnlyEnforceIf(both_2)
            model.AddBoolOr([is_home[m, t2_idx].Not(), is_away[m, t1_idx].Not()]).OnlyEnforceIf(both_2.Not())

            either = model.NewBoolVar(f"vs_match_{t1}_{t2}_{m}")
            model.AddBoolOr([both_1, both_2]).OnlyEnforceIf(either)
            model.AddBoolAnd([both_1.Not(), both_2.Not()]).OnlyEnforceIf(either.Not())

            vs_match_flags_storage[vs_match_idx] = (both_1, both_2, either)
            vs_match_flags.append(either)

        model.Add(sum(vs_match_flags) == expected)




#############################################################################
# Optimizations
# add a function for the stuff we want the solver to do less of
# qwerqwerqwer

# optimizing to reduce idle time
idle_cost_vars = []

for t_idx in range(len(teams)):
    for w in weekend_idxs:
        #for (date, field) in set((m[1], m[2]) for m in matches if m[0] == w):
        relevant_matches = [m for m in matches if m[0] == w]
        if not relevant_matches:
            continue

        # Use match_time_ints instead of redefining conditional time vars
        conditional_times_for_min = [match_time_ints_for_min[m, t_idx] for m in relevant_matches]
        conditional_times_for_max = [match_time_ints_for_max[m, t_idx] for m in relevant_matches]
        play_flags = [is_playing[m, t_idx] for m in relevant_matches]
        time_indices = [m[4] for m in relevant_matches]
        max_time = max(time_indices)

        # Vars for earliest, latest, and idle
        earliest = model.NewIntVar(0, max_time_idx+1, f"earliest_{t_idx}_{w}")
        latest = model.NewIntVar(0, max_time_idx+1, f"latest_{t_idx}_{w}")

        # Use Min/Max over precomputed time variables
        model.AddMinEquality(earliest, conditional_times_for_min)
        model.AddMaxEquality(latest, conditional_times_for_max)

        plays_twice_or_more = model.NewBoolVar(f"plays_twice_or_more_{t_idx}_{w}")
        model.Add(sum(play_flags) >= 2).OnlyEnforceIf(plays_twice_or_more)
        model.Add(sum(play_flags) < 2).OnlyEnforceIf(plays_twice_or_more.Not())

        # Idle = latest - earliest - 1 (only non-zero if playing ≥ 2 times)
        idle = model.NewIntVar(0, max_time_idx+1, f"idle_{t_idx}_{w}")
        model.Add(idle == latest - earliest - 1).OnlyEnforceIf(plays_twice_or_more)
        model.Add(idle == 0).OnlyEnforceIf(plays_twice_or_more.Not())

        idle_cost_vars.append(idle)

# even out the cost of setting up the field
setup_flags = {}

for m in matches:
    if m[6] != 1:  # Only matches that require field setup
        continue

    for t_idx in range(len(teams)):
        if t_idx not in setup_flags:
            setup_flags[t_idx] = []
        setup_flag = model.NewBoolVar(f"field_setup_{m}_{t_idx}")
        model.Add(setup_flag == is_home[m, t_idx])  # True iff team is home
        setup_flags[t_idx].append(setup_flag)

excess_team_setup_costs = []

for t_idx in range(len(teams)):
    total_setups = model.NewIntVar(0, len(matches), f"total_setups_{t_idx}")
    model.Add(total_setups == sum(setup_flags.get(t_idx, [])))

    overage = model.NewIntVar(0, len(matches), f"excess_setups_{t_idx}")

    # Constraint: overage = max(0, total_setups - 2)
    tmp = model.NewIntVar(-len(matches), len(matches), f"tmp_overage_{t_idx}")
    model.Add(tmp == total_setups - 2)
    model.AddMaxEquality(overage, [tmp, model.NewConstant(0)])

    excess_team_setup_costs.append(overage)

# Minimize total idle time across the season
model.Minimize(sum(idle_cost_vars) + 10 * sum(excess_team_setup_costs))

# currently none

#############################################################################
# Solve it

solver = cp_model.CpSolver()
solver.parameters.max_time_in_seconds = 900  # 10 minutes   ### 900  # 10 minutes     3600  # 1 hour
solver.parameters.num_search_workers = 8  # Use multicore
#solver.parameters.search_branching = cp_model.AUTOMATIC
solver.parameters.linearization_level = 0
solver.parameters.cp_model_presolve = True


start = datetime.datetime.now()
print(f"Starting solution process at {start}")
status = solver.Solve(model)
end = datetime.datetime.now()
delta = end - start
print(f"Solver status: {solver.StatusName(status)}")
print(f"Finished at {end}, duration: {delta}")


#############################################################################
# Turn it back into a schedule df so we can do stuff with it
#
if solver.StatusName(status) not in ("UNKNOWN", "INFEASIBLE"):
  schedule_rows = []

  for m in matches:
      home_idx = solver.Value(home_team[m])
      away_idx = solver.Value(away_team[m])

      weekend_idx, date, field, time, time_idx, max_time_idx, field_setup = m
      schedule_rows.append({
          "weekend_idx": weekend_idx,
          "date": date,
          "field": field,
          "time": time,
          "home_team": teams[home_idx],
          "away_team": teams[away_idx],
          "time_idx": time_idx,
          "max_time_idx": max_time_idx,
          "field_setup": field_setup,
      })

  schedule_df = pd.DataFrame(schedule_rows)
  schedule_df.sort_values(["weekend_idx", "date", "field", "time"], inplace=True)
  schedule_df.reset_index(drop=True, inplace=True)

schedule_report = ''

notify_done()



# Step 1: Map team names to team indexes
team_to_idx = {name: idx for idx, name in enumerate(teams)}
idx_to_team = {idx: name for name, idx in team_to_idx.items()}

# Step 2: Build long-form schedule of all games (team, weekend_idx, time_idx)
home_rows = schedule_df[["weekend_idx", "home_team", "time_idx"]].rename(columns={"home_team": "team"})
away_rows = schedule_df[["weekend_idx", "away_team", "time_idx"]].rename(columns={"away_team": "team"})
long_df = pd.concat([home_rows, away_rows], ignore_index=True)

# Map team names to indexes
long_df["team_idx"] = long_df["team"].map(team_to_idx)

# Step 3: Compute idle time per (weekend_idx, team_idx)
def compute_idle(group):
    if len(group) <= 1:
        return 0
    idle = group["time_idx"].max() - group["time_idx"].min() - (len(group) - 1)
    return max(0, idle)

idle_df = long_df.groupby(["weekend_idx", "team_idx"]).apply(compute_idle).reset_index()
idle_df.columns = ["weekend_idx", "team_idx", "max_idle_slots"]

# Step 4: Pivot to get weekend_idx rows and team_idx columns
idle_matrix = idle_df.pivot(index="weekend_idx", columns="team_idx", values="max_idle_slots").fillna(0).astype(int)

# Optional: sort columns (team indexes)
idle_matrix = idle_matrix.sort_index(axis=1)

schedule_report += '\n\n\n' + idle_matrix.to_string()

display(idle_matrix)



import pandas as pd

print(len(schedule_df))
#display(schedule_df)


import numpy as np


# Initialize 14x14 matrix of zeros
vs_matrix = np.zeros((num_teams, num_teams), dtype=int)

# Count matchups
for _, row in schedule_df.iterrows():
    home_idx = team_to_idx[row["home_team"]]
    away_idx = team_to_idx[row["away_team"]]
    vs_matrix[home_idx][away_idx] += 1
    vs_matrix[away_idx][home_idx] += 1  # symmetric

# Turn into a DataFrame with team indexes as labels
vs_df = pd.DataFrame(vs_matrix, index=range(num_teams), columns=range(num_teams))

# Optional: add team names as labels
# vs_df = pd.DataFrame(vs_matrix, index=teams, columns=teams)

schedule_report += '\n\n\n' + vs_df.to_string()


display(vs_df)

schedule_df

weekend_idxs = sorted(schedule_df["weekend_idx"].unique())
play_counts = pd.DataFrame(0, index=teams, columns=weekend_idxs)

# Count home and away appearances per team per weekend
for _, row in schedule_df.iterrows():
    w = row["weekend_idx"]
    play_counts.at[row["home_team"], w] += 1
    play_counts.at[row["away_team"], w] += 1

# Optional: sort rows/columns
play_counts = play_counts.sort_index().sort_index(axis=1)


schedule_report += '\n\n\n' + play_counts.to_string()


display(play_counts)

import pandas as pd

# Group play counts by (team, weekend_idx, field)
play_records = []

for _, row in schedule_df.iterrows():
    for team_col in ["home_team", "away_team"]:
        play_records.append({
            "team": row[team_col],
            "weekend_idx": row["weekend_idx"],
            "field": row["field"]
        })

play_df = pd.DataFrame(play_records)

# Pivot table: rows=team, columns=(weekend_idx, field), values=play counts
pivot = play_df.pivot_table(
    index="team",
    columns=["weekend_idx", "field"],
    aggfunc="size",
    fill_value=0
)

# Optional: sort index and columns
pivot = pivot.sort_index().sort_index(axis=1)

schedule_report += '\n\n\n' + pivot.to_string()

# Display result
display(pivot)

teams

schedule_df[schedule_df.weekend_idx == 1]

import pandas as pd

# Count home games per team
home_counts = schedule_df["home_team"].value_counts().rename("home_games")

# Count away games per team
away_counts = schedule_df["away_team"].value_counts().rename("away_games")

# Combine into one DataFrame
home_away_df = pd.concat([home_counts, away_counts], axis=1).fillna(0).astype(int)

# Add total and balance
home_away_df["total_games"] = home_away_df["home_games"] + home_away_df["away_games"]
home_away_df["home_away_diff"] = home_away_df["home_games"] - home_away_df["away_games"]

# Optional: sort by team name or imbalance
home_away_df = home_away_df.sort_index()

schedule_report += '\n\n\n' + home_away_df.to_string()

display(home_away_df)

field_setup_df = schedule_df.copy()
field_setup_df = field_setup_df[["home_team", "field_setup"]]
field_setup_df = field_setup_df[field_setup_df["field_setup"] == 1]
grouped_field_setup = field_setup_df.groupby("home_team").count()


schedule_report += '\n\n\n' + grouped_field_setup.to_string()

display(grouped_field_setup)

# Removed before sync 2025-05-09


def extract_sheet_id(sheet_url):
    match = re.search(r'/d/([a-zA-Z0-9-_]+)', sheet_url)
    if match:
        return match.group(1)
    else:
        raise ValueError("Invalid Google Sheet URL")
output_sheet_id = extract_sheet_id(output_google_sheet_url)

# Removed before sync 2025-05-09


if not FINAL_EXPORT:
  assert False

########################################################
# getting output to sheets
# sample code from here https://chatgpt.com/c/6816dab8-04f0-8004-86a6-a1849748f752
# qwerqwerqwer

from gspread_pandas import Spread, Client
from gspread_dataframe import set_with_dataframe
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# Removed before sync 2025-05-09

drive_service_key_full_path = drive_service_key_path + drive_service_key_name

path_to_check = ''
os.listdir(drive_service_key_path)
os.path.exists(drive_service_key_full_path)

# Removed before sync 2025-05-09

creds = ServiceAccountCredentials.from_json_keyfile_name(drive_service_key_full_path, scope)
client = gspread.authorize(creds)

export_df = schedule_df.copy()
export_df.drop(['time_idx', 'max_time_idx', 'weekend_idx'], axis=1, inplace=True)

############################################################
# access and update the sheet
spreadsheet = client.open_by_key(output_sheet_id)
schedule_tab_name = 'auto_export_schedule'
schedule_sheet = spreadsheet.worksheet(schedule_tab_name)  # this must already exist

# Clear and update the sheet
schedule_sheet.clear()
set_with_dataframe(schedule_sheet, export_df)

report_tab_name = 'schedule_reports'
report_sheet = spreadsheet.worksheet(report_tab_name)  # this must already exist
report_sheet.clear()

lines = schedule_report.strip().split('\n')
report_df = pd.DataFrame({'report': lines})

set_with_dataframe(report_sheet, report_df)
# Convert to list of lists for Google Sheets (each line as its own row)
#report_rows = [[line] for line in lines]

# Push to the sheet starting at A1
#report_sheet.update("A1", report_rows)

print("✅ Data exported")

# schedule_report

for team_idx, team in enumerate(teams):
    print(f"{team_idx}: {team}")





































